{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' plant that grows on roots, leaflets or other organic materials, such as leaves, flowers, foliage and fiber. Each tomato plant has an individual physiology. All four of these organs are interconnected and are capable of delivering nutrients. Many, if not all, of these organs are responsible for the growth of plants. The only real part of these structures that are not actually part of the living tissue (weighing) actually works.\\n\\nThe key to making tomato grows faster and more productive is to grow on a mature tomato. This means doing all of the following things â€“ keeping temperatures, watering thoroughly, having plenty of food, eating right and with adequate water.\\n\\n1. Keep temperature and watering relatively high\\n\\n2. Keep watering at'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\", \n",
    "    #tiiuae/falcon-7b-instruct\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\":150,\n",
    "    },\n",
    ")\n",
    "\n",
    "chain = prompt |llm\n",
    "chain.invoke({\"word\": \"tomato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"[INST]what is the meaning of {word}[/INST]\")\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id = \"mistralai/Mistral-7B-instruct-v0.1\",\n",
    "    model_kwargs = {\n",
    "        \"max_new_tokens\": 250,\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = prompt |llm\n",
    "chain.invoke({\"word\": \"potato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "llm = GPT4All(\n",
    "    model=\"./falcon.bin\",\n",
    "    \n",
    ")\n",
    "\n",
    "chain = prompt |llm\n",
    "chain.invoke({\"word\": \"tomato\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
